{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. \n",
    "\n",
    "\n",
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from gram2vec.featurizers import GrammarVectorizer\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Author:\n",
    "    \"\"\"\n",
    "    Stores author information in an easy to work with format\n",
    "    \n",
    "    :param author_id: unique author id\n",
    "    :param fixed_texts: list of author documents with regex fixes\n",
    "    :param raw_texts: list of author documents without regex fixes\n",
    "    :param discourse_types: list of discourse types\n",
    "    \n",
    "    Note: fixed_docs, raw_docs, and discourse_types are all 1 - 1 corresponding\n",
    "    \"\"\"\n",
    "    author_id:str\n",
    "    fixed_texts:list[str]\n",
    "    raw_texts:list[str]\n",
    "    discourse_types:list[str]\n",
    "    \n",
    "    def get_token_counts(self) -> list[int]:\n",
    "        return [len(word_tokenize(author_doc)) for author_doc in self.fixed_texts]\n",
    "    \n",
    "    def get_total_docs(self) -> int:\n",
    "        return len(self.fixed_texts)\n",
    "    \n",
    "    def counted_dicourse_types(self):\n",
    "        return Counter(self.discourse_types)\n",
    "        \n",
    "def load_preproccessed_json(path:str) -> dict[str, list[dict]]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        data = json.load(fin)\n",
    "        return data\n",
    "\n",
    "def extract_from_dict(author_entry:dict, to_extract:str) -> list[str]:\n",
    "    return [entry[to_extract] for entry in author_entry]\n",
    "    \n",
    "def create_author_list(preprocessed_data:dict[str, list[dict]]) -> list[Author]:\n",
    "    \"\"\"\n",
    "    Converts the preprocessed_data.json data into a list of Author objects\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    for author_id in preprocessed_data.keys():\n",
    "        author_entry = preprocessed_data[author_id]\n",
    "        fixed_texts = extract_from_dict(author_entry,\"fixed_text\")\n",
    "        raw_texts = extract_from_dict(author_entry,\"raw_text\")\n",
    "        discourse_types = extract_from_dict(author_entry,\"discourse_type\")\n",
    "            \n",
    "        authors.append(Author(author_id, fixed_texts, raw_texts, discourse_types))\n",
    "        \n",
    "    return authors\n",
    "\n",
    "def get_total_doc_count(authors:list[Author]) -> int:\n",
    "    \"\"\"Computes total number of documents\"\"\"\n",
    "    return sum([author.get_total_docs() for author in authors])\n",
    "\n",
    "def get_total_author_count(authors:list[Author]) -> int:\n",
    "    \"\"\"Computes total number of authors\"\"\"\n",
    "    return len(authors)\n",
    "\n",
    "def get_doc_token_stats(authors:list[Author]) -> tuple[float, float]:\n",
    "    \"\"\"Gets the mean and std of tokens per document\"\"\"\n",
    "    all_doc_token_counts = []\n",
    "    for author in authors:\n",
    "        all_doc_token_counts.extend(author.get_token_counts())\n",
    "    return np.mean(all_doc_token_counts), np.std(all_doc_token_counts)\n",
    "    \n",
    "def get_author_token_stats(authors:list[Author]) -> tuple[float, float]:\n",
    "    \"\"\"Gets the mean and std of tokens per author\"\"\"\n",
    "    author_to_token_counts = defaultdict(list)\n",
    "    for author in authors:\n",
    "        author_to_token_counts[author.author_id] = sum(author.get_token_counts())\n",
    "    values = list(author_to_token_counts.values())\n",
    "    return np.mean(values), np.std(values)\n",
    "    \n",
    "def get_author_doc_stats(authors:list[Author]) -> tuple[float, float, int, int]:\n",
    "    \"\"\"Gets mean, std, min, max of documents per author\"\"\"\n",
    "    doc_counts = [author.get_total_docs() for author in authors]\n",
    "    return np.mean(doc_counts), np.std(doc_counts), min(doc_counts), max(doc_counts)\n",
    "    \n",
    "#TODO: rewrite discourse type counting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_preproccessed_json(\"data/pan22/preprocessed/preprocessed_data.json\")\n",
    "all_authors = create_author_list(data)\n",
    "\n",
    "total_doc_count = get_total_doc_count(all_authors)\n",
    "total_author_count = get_total_author_count(all_authors)\n",
    "\n",
    "avg_tokens_per_doc, std_tokens_per_doc = get_doc_token_stats(all_authors)\n",
    "avg_tokens_per_author, std_tokens_per_author = get_author_token_stats(all_authors)\n",
    "author_avg, author_std, author_min, author_max = get_author_doc_stats(all_authors)\n",
    "\n",
    "\n",
    "with open(\"stats.txt\", \"w\") as fout:\n",
    "    fout.write(f\"\"\"\n",
    "Total docs: {total_doc_count}\n",
    "Total authors: {total_author_count}\n",
    "Mean tokens per doc: {avg_tokens_per_doc:.2f}\n",
    "Std tokens per doc: {std_tokens_per_doc:.2f}\n",
    "Mean tokens per author: {avg_tokens_per_author:.2f}\n",
    "Std tokens per author: {std_tokens_per_author:.2f}\n",
    "Mean # of docs: {author_avg:.2f}\n",
    "Std # of docs: {author_std:.2f}\n",
    "Min # of docs: {author_min}\n",
    "Max # of docs: {author_max}\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
