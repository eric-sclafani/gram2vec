{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import spacy\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from contextlib import contextmanager\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy import displacy\n",
    "\n",
    "class Dummy:\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def from_docs(cls,docs):\n",
    "        \n",
    "        docs = cls.nlp.pipe(docs)\n",
    "        return docs\n",
    "    \n",
    "    \n",
    "\n",
    "def _load_data(data_dir:str) -> pd.DataFrame:\n",
    "        \"\"\"Loads a directory of .jsonl files\"\"\"\n",
    "    \n",
    "        return pd.read_json(data_dir, lines=True)\n",
    "    \n",
    "    \n",
    "pan_docs = _load_data(\"data/pan22/preprocessed/pan22_preprocessed.jsonl\")[\"fullText\"].to_list()\n",
    "    \n",
    "\n",
    "#! getting a list of all morph tags from pan. May need to find them on internet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbalizer import Verbalizer\n",
    "\n",
    "datapaths = {\n",
    "        \"hrs\": \"data/hrs_release_03-20-23/raw/hrs1_03-20-23_background_bgg_350_anonymized.jsonl\",\n",
    "        \"pan\":\"data/pan22/preprocessed/pan22_preprocessed.jsonl\"\n",
    "    }\n",
    "    \n",
    "pan_verb = Verbalizer(datapaths[\"pan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document uses more POS Unigram: CCONJ than the average\n",
      "This document uses more POS Unigram: NUM than the average\n",
      "This document uses more POS Bigram: ('AUX', 'VERB') than the average\n",
      "This document uses more POS Bigram: ('NOUN', 'CCONJ') than the average\n",
      "This document uses more Function word: i than the average\n",
      "This document uses more Function word: its than the average\n",
      "This document uses more Function word: were than the average\n",
      "This document uses more Function word: being than the average\n",
      "This document uses more Function word: an than the average\n",
      "This document uses more Function word: but than the average\n",
      "This document uses more Function word: by than the average\n",
      "This document uses more Function word: over than the average\n",
      "This document uses more Function word: some than the average\n",
      "This document uses more Letter: o than the average\n",
      "This document uses more Letter: x than the average\n",
      "This document uses more Letter: N than the average\n",
      "This document uses more Dependency label: auxpass than the average\n",
      "This document uses more Dependency label: cc than the average\n",
      "This document uses more Dependency label: nsubjpass than the average\n",
      "This document uses more Dependency label: nummod than the average\n",
      "This document uses more Mixed Bigram: ('to', 'NOUN') than the average\n",
      "This document uses more Mixed Bigram: ('an', 'NOUN') than the average\n",
      "This document uses more Morphology tag: Part than the average\n",
      "This document uses more Morphology tag: Ger than the average\n"
     ]
    }
   ],
   "source": [
    "pan_ids = {\n",
    "    \"authors\":[\n",
    "        \"en_110\",\n",
    "        \"en_100\",\n",
    "        \"en_113\"\n",
    "        ],\n",
    "    \"docs\":[\n",
    "        \"ed5ec66c-d70f-11ed-8cc6-76349838619d\",\n",
    "        \"ed73dd7c-d70f-11ed-8cc6-76349838619d\",\n",
    "        \"ee46a040-d70f-11ed-8cc6-76349838619d\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# API will change\n",
    "v = pan_verb.verbalize(\"ee46a040-d70f-11ed-8cc6-76349838619d\", to_verbalize=\"documentID\")\n",
    "for n in v[\"verbalized\"]:\n",
    "    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (3138,), (1,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m hrs_ids \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m\"\u001b[39m:[\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m6e34df08-ed37-5941-85a3-d590511781f9\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m hrs_verb \u001b[39m=\u001b[39m Verbalizer(datapaths[\u001b[39m\"\u001b[39;49m\u001b[39mhrs\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/python/pausit-eval/gram2vec/gram2vec/verbalizer.py:17\u001b[0m, in \u001b[0;36mVerbalizer.__init__\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_data(data_dir)\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_docs_df()\n\u001b[0;32m---> 17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauthor_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_author_df()\n",
      "File \u001b[0;32m~/python/pausit-eval/gram2vec/gram2vec/verbalizer.py:57\u001b[0m, in \u001b[0;36mVerbalizer._make_author_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m author_ids_to_avs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m author_id \u001b[39min\u001b[39;00m author_ids:\n\u001b[0;32m---> 57\u001b[0m     author_doc_entries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_author_docs(author_id)\n\u001b[1;32m     58\u001b[0m     author_doc_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclude_columns(author_doc_entries, cols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdocumentID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthorIDs\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     59\u001b[0m     author_ids_to_avs[author_id] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(author_doc_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/python/pausit-eval/gram2vec/gram2vec/verbalizer.py:37\u001b[0m, in \u001b[0;36mVerbalizer.get_author_docs\u001b[0;34m(self, author_id)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_author_docs\u001b[39m(\u001b[39mself\u001b[39m, author_id:\u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Retrieves an authors documents from self.docs_df\"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs_df\u001b[39m.\u001b[39mloc[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocs_df[\u001b[39m'\u001b[39;49m\u001b[39mauthorIDs\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m author_id]\n",
      "File \u001b[0;32m~/python/pausit-eval/venv/lib/python3.11/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/python/pausit-eval/venv/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[0;32m~/python/pausit-eval/venv/lib/python3.11/site-packages/pandas/core/series.py:6097\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6094\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6096\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6097\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6099\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/python/pausit-eval/venv/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:263\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, (np\u001b[39m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lvalues) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(rvalues):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    264\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLengths must match to compare\u001b[39m\u001b[39m\"\u001b[39m, lvalues\u001b[39m.\u001b[39mshape, rvalues\u001b[39m.\u001b[39mshape\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    268\u001b[0m     (\u001b[39misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[39mor\u001b[39;00m right \u001b[39mis\u001b[39;00m NaT)\n\u001b[1;32m    269\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    270\u001b[0m ):\n\u001b[1;32m    271\u001b[0m     \u001b[39m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     res_values \u001b[39m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[0;31mValueError\u001b[0m: ('Lengths must match to compare', (3138,), (1,))"
     ]
    }
   ],
   "source": [
    "hrs_ids = {\n",
    "    \"authors\":[\n",
    "        \"6e34df08-ed37-5941-85a3-d590511781f9\",\n",
    "        \"2de9ffc8-deb7-53fb-b4db-a6ab9af0bbf1\",\n",
    "        \"dd47dc76-ad48-5d40-b1c8-71191522f15d\"\n",
    "        ],\n",
    "    \"docs\":[\n",
    "        \"322d6728-39de-54f1-9568-da4352aca6f7\",\n",
    "        \"a4651175-6e96-5075-832f-fd818873f669\",\n",
    "        \"574f9067-0f11-5753-8036-9d4112106301\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "#hrs_verb = Verbalizer(datapaths[\"hrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
